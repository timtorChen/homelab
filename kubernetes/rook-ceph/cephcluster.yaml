---
apiVersion: ceph.rook.io/v1
kind: CephCluster
metadata:
  namespace: rook-ceph
  name: main
spec:
  cephVersion:
    image: quay.io/ceph/ceph:v18.2.2
  dataDirHostPath: /var/lib/rook
  mon:
    count: 3
    allowMultiplePerNode: false
  mgr:
    count: 2
    modules:
      - name: telemetry
        enabled: false
      - name: nfs
        enabled: false
      - name: pg_autoscaler
        enabled: true
  storage:
    useAllNodes: false
    useAllDevices: false
    nodes:
      - name: amethyst-nuc11tnhi50l-1
        devices:
          - config:
              deviceClass: nvme
            name: /dev/disk/by-id/nvme-Micron_7450_MTFDKBA960TFR_22473CDCF214
          - config:
              deviceClass: hdd
            name: /dev/disk/by-id/ata-ST4000VN008-2DR166_ZDH96900
      - name: amethyst-nuc11tnhi50l-2
        devices:
          - config:
              deviceClass: nvme
            name: /dev/disk/by-id/nvme-Micron_7450_MTFDKBA960TFR_22473CDD4746
          - config:
              deviceClass: hdd
            name: /dev/disk/by-id/ata-ST4000VN008-2DR166_ZDH95W58
      - name: amethyst-nuc11tnhi50l-3
        devices:
          - config:
              deviceClass: nvme
            name: /dev/disk/by-id/nvme-Micron_7450_MTFDKBA960TFR_22473CDCF204
          - config:
              deviceClass: hdd
            name: /dev/disk/by-id/ata-ST4000VN008-2DR166_ZDH93J1E
  resources:
    osd:
      requests:
        memory: 2Gi
    mon:
      requests:
        memory: 1Gi
    mgr:
      requests:
        memory: 512Mi
    crashcollector:
      requests:
        memory: 60Mi
    exporter:
      limits:
        memory: 64Mi
      requests:
        memory: 64Mi
  network:
    provider: host
  dashboard:
    enabled: true
  monitoring:
    enabled: true
    interval: 1m
  disruptionManagement:
    managePodBudgets: true
    osdMaintenanceTimeout: 30 # max pg recovery duration
    pgHealthCheckTimeout: 1 # wait 1 min after pgs become healthy
